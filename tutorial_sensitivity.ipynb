{
 "metadata": {
  "name": "",
  "signature": "sha256:3dda406831a8734acdf8f9da27a40303f465e0f57f224a193f1f6507b12e9737"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classification Model Parameters -- Sensitivity Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "In the *chap_tutorial_searchlight* we made a first attempt at localizing\n",
      "information in the brain that is relevant to a particular classification\n",
      "analyses. While we were relatively successful, we experienced some problems and\n",
      "also had to wait quite a bit. Here we want to look at another approach to\n",
      "localization. To get started, we pre-process the data as we have done before\n",
      "and perform volume averaging to get a single sample per stimulus category and\n",
      "original experiment session."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mvpa2.tutorial_suite import *\n",
      "ds = get_raw_haxby2001_data(roi='brain')\n",
      "print ds.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1452, 39912)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly_detrend(ds, polyord=1, chunks_attr='chunks')\n",
      "zscore(ds, param_est=('targets', ['rest']))\n",
      "ds = ds[ds.sa.targets != 'rest']\n",
      "run_averager = mean_group_sample(['targets', 'chunks'])\n",
      "ds = ds.get_mapped(run_averager)\n",
      "print ds.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(96, 39912)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "A searchlight analysis on this dataset would look exactly as we have seen in\n",
      "*chap_tutorial_searchlight*, but it would take a bit longer due to a\n",
      "higher number of samples. The error map that is the result of a searchlight\n",
      "analysis only offers an approximate localization. First, it is smeared by the\n",
      "overlapping spheres and second the sphere-shaped ROIs probably do not reflect\n",
      "the true shape and extent of functional subregions in the brain. Therefore, it\n",
      "mixes and matches things that might not belong together. This can be mitigated\n",
      "to some degree by using more clever searchlight algorithms (see\n",
      "*example_searchlight_surf*).  But it would also be much nicer if we were\n",
      "able to obtain a per-feature measure, where each value can really be attributed\n",
      "to the respective feature and not just to an area surrounding it."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "It's A Kind Of Magic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "One way to get such a measure is to inspect the classifier itself. Each\n",
      "classifier creates a model to map from the training data onto the\n",
      "respective target values. In this model, classifiers typically associate\n",
      "some sort of weight with each feature that is an indication of its impact\n",
      "on the classifiers decision. How to get this information from a\n",
      "classifier will be the topic of this tutorial.\n",
      "\n",
      "However, if we want to inspect a trained classifier, we first have to train\n",
      "one. But hey, we have a full brain dataset here with almost 40k features.\n",
      "Will we be able to do that? Well, let's try (and hope that there is still a\n",
      "warranty on the machine you are running this on...).\n",
      "\n",
      "We will use a simple cross-validation procedure with a linear support\n",
      "vector machine.  We will also be interested in summary statistics of the\n",
      "classification, a confusion matrix in our case of classification:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = LinearCSVMC()\n",
      "cvte = CrossValidation(clf, NFoldPartitioner(),\n",
      "                       enable_ca=['stats'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Ready, set, go!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = cvte(ds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "That was surprisingly quick, wasn't it? But was it any good?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.round(cvte.ca.stats.stats['ACC%'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "26.0\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cvte.ca.stats.matrix "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 1 2 3 0 1 1 1]\n",
        " [1 2 2 0 2 3 3 1]\n",
        " [5 3 3 0 4 3 0 2]\n",
        " [3 2 0 5 0 0 0 1]\n",
        " [0 3 1 0 3 2 0 0]\n",
        " [0 0 0 0 0 0 1 0]\n",
        " [0 1 4 3 2 1 7 3]\n",
        " [2 0 0 1 1 2 0 4]]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Well, the accuracy is not exactly at a chance level, but the confusion matrix doesn't\n",
      "seem to have any prominent diagonal. It looks like, although we can easily\n",
      "train a support vector machine on the full brain dataset, it cannot construct\n",
      "a reliably predicting model.  At least we are in the lucky situation to already know\n",
      "that there is some signal in the data, hence we can attribute this failure\n",
      "to the classifier. In most situations it would be as likely that there is\n",
      "actually no signal in the data...\n",
      "\n",
      "Often people claim that classification performance improves with\n",
      "[feature selection](http://pymvpa.org/glossary.html#term-feature-selection). If we can reduce the dataset to the important ones,\n",
      "the classifier wouldn't have to deal with all the noise anymore. A simple\n",
      "approach would be to compute a full-brain ANOVA and only go with the voxels\n",
      "that show some level of variance between categories. From the\n",
      "*chap_tutorial_searchlight* we know how to compute the desired F-scores\n",
      "and we could use them to manually select features with some threshold. However,\n",
      "PyMVPA offers a more convenient way -- feature selectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsel = SensitivityBasedFeatureSelection(\n",
      "           OneWayAnova(),\n",
      "           FixedNElementTailSelector(500, mode='select', tail='upper'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "The code snippet above configures such a selector. It uses an ANOVA measure\n",
      "to select 500 features with the highest F-scores. There\n",
      "are a lot more ways to perform the selection, but we will go with this one\n",
      "for now. The [SensitivityBasedFeatureSelection](http://pymvpa.org/generated/mvpa2.featsel.base.SensitivityBasedFeatureSelection.html#mvpa2-featsel-base-sensitivitybasedfeatureselection)\n",
      "instance is yet another [processing object](http://pymvpa.org/glossary.html#term-processing-object) that can be called with a\n",
      "dataset to perform the feature selection:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsel.train(ds)\n",
      "ds_p = fsel(ds)\n",
      "print ds_p.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(96, 500)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/dist-packages/numpy/lib/utils.py:134: DeprecationWarning: `fprob` is deprecated!\n",
        "fprob is deprecated in scipy 0.14, use stats.f.sf or special.fdtrc instead\n",
        "\n",
        "  warnings.warn(depdoc, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "This is the dataset we wanted, so we can rerun the cross-validation and see\n",
      "if it helped. But first, take a step back and look at this code snippet again.\n",
      "There is an object that gets called with a dataset and returns a dataset. You\n",
      "cannot prevent noticing the striking similarity between a measure in PyMVPA or\n",
      "a mapper. And yes, feature selection procedures are also\n",
      "[processing object](http://pymvpa.org/glossary.html#term-processing-object)s and work just like measures or mappers. Now back\n",
      "to the analysis:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = cvte(ds_p)\n",
      "print np.round(cvte.ca.stats.stats['ACC%'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "79.2\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cvte.ca.stats.matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 5  0  3  0  0  3  0  2]\n",
        " [ 0 11  0  0  0  0  0  0]\n",
        " [ 0  0  7  0  0  1  0  0]\n",
        " [ 2  1  0 12  0  0  0  0]\n",
        " [ 0  0  0  0 12  0  0  0]\n",
        " [ 2  0  1  0  0  8  0  0]\n",
        " [ 0  0  1  0  0  0 12  1]\n",
        " [ 3  0  0  0  0  0  0  9]]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Yes! We did it. Almost 80% correct classification for an 8-way\n",
      "classification and the confusion matrix has a strong diagonal. Apparently,\n",
      "the ANOVA-selected features were the right ones."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "If you are not yet screaming or haven't started composing an email to the\n",
      "PyMVPA mailing list pointing to a major problem in the tutorial, you need\n",
      "to reconsider what we have just done. Why is this wrong?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "\n",
      "\n",
      "Let's repeat this analysis on a subset of the data. We select only `bottle`\n",
      "and `shoe` samples. In the analysis we just did, they are relatively often\n",
      "confused by the classifier. Let's see how the full brain SVM performs on\n",
      "this binary problem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bin_demo = ds[np.array([i in ['bottle', 'shoe'] for i in ds.sa.targets])]\n",
      "results = cvte(bin_demo)\n",
      "print np.round(cvte.ca.stats.stats['ACC%'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "62.5\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Not much, but that doesn't surprise. Let's see what effect our ANOVA-based\n",
      "feature selection has"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsel.train(bin_demo)\n",
      "bin_demo_p = fsel(bin_demo)\n",
      "results = cvte(bin_demo_p)\n",
      "print cvte.ca.stats.stats[\"ACC%\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Wow, that is a jump. Perfect classification performance, even though the\n",
      "same categories couldn't be distinguished by the same classifier, when\n",
      "trained on all eight categories. I guess, it is obvious that our way of\n",
      "selecting features is somewhat fishy -- if not illegal. The ANOVA measure\n",
      "uses the full dataset to compute the F-scores, hence it determines which\n",
      "features show category differences in the whole dataset, including our\n",
      "supposed-to-be independent testing data. Once we have found these\n",
      "differences, we are trying to rediscover them with a classifier.  Being able\n",
      "to do that is not surprising, and precisely constitutes the *double-dipping*\n",
      "procedure. As a result, both the obtained prediction\n",
      "accuracy and the created model are potentially completely meaningless."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Thanks For The Fish"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "To implement an ANOVA-based feature selection *properly* we have to do it on\n",
      "the training dataset **only**. The PyMVPA way of doing this is via a\n",
      "[FeatureSelectionClassifier](http://pymvpa.org/generated/mvpa2.clfs.meta.FeatureSelectionClassifier.html#mvpa2-clfs-meta-featureselectionclassifier):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fclf = FeatureSelectionClassifier(clf, fsel)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "This is a [meta-classifier](http://pymvpa.org/glossary.html#term-meta-classifier) and it just needs two things: A basic\n",
      "classifier to do the actual classification work and a feature selection\n",
      "object. We can simply re-use the object instances we already had. Now we\n",
      "got a meta-classifier that can be used just as any other classifier. Most\n",
      "importantly we can plug it into a cross-validation procedure (almost\n",
      "identical to the one we had in the beginning)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cvte = CrossValidation(fclf, NFoldPartitioner(),\n",
      "                       enable_ca=['stats'])\n",
      "results = cvte(bin_demo)\n",
      "print np.round(cvte.ca.stats.stats['ACC%'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "70.8\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "This is a lot worse and a lot closer to the truth -- or a so-called\n",
      "[unbiased estimate](http://pymvpa.org/glossary.html#term-unbiased-estimate) of the generalizability of the classifier model.\n",
      "Now we can also run this improved procedure on our original 8-category\n",
      "dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = cvte(ds)\n",
      "print np.round(cvte.ca.stats.stats['ACC%'], 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "78.1\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cvte.ca.stats.matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 5  0  2  0  0  4  0  2]\n",
        " [ 0 10  0  0  0  0  0  0]\n",
        " [ 0  0  8  0  0  1  0  0]\n",
        " [ 2  2  0 12  0  0  0  0]\n",
        " [ 0  0  0  0 12  0  0  0]\n",
        " [ 1  0  1  0  0  7  0  0]\n",
        " [ 0  0  1  0  0  0 12  1]\n",
        " [ 4  0  0  0  0  0  0  9]]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "That is still a respectable accuracy for an 8-way classification and the\n",
      "confusion table also confirms this."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dissect The Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "But now back to our original goal: getting the classifier's opinion about\n",
      "the importance of features in the dataset. With the approach we have used\n",
      "above, the classifier is trained on 500 features. We can only have its\n",
      "opinion about those. Although this is just few times larger than a typical\n",
      "searchlight sphere, we have lifted the spatial constraint of\n",
      "searchlights -- these features can come from all over an ROI.\n",
      "\n",
      "However, we still want to consider more features, so we are changing the\n",
      "feature selection to retain more."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsel = SensitivityBasedFeatureSelection(\n",
      "           OneWayAnova(),\n",
      "           FractionTailSelector(0.05, mode='select', tail='upper'))\n",
      "fclf = FeatureSelectionClassifier(clf, fsel)\n",
      "cvte = CrossValidation(fclf, NFoldPartitioner(),\n",
      "                       enable_ca=['stats'])\n",
      "results = cvte(ds)\n",
      "print cvte.ca.stats.stats['ACC%'] >= 78.1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.round(cvte.ca.stats.stats['ACC%'], 1)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "69.8\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "A drop of 8% in accuracy on about 4 times the number of features. This time\n",
      "we asked for the top 5% of F-scores.\n",
      "\n",
      "But how do we get the weights, finally? In PyMVPA many classifiers\n",
      "are accompanied with so-called [sensitivity analyzer](http://pymvpa.org/glossary.html#term-sensitivity-analyzer)s. This is an\n",
      "object that knows how to get them from a particular classifier type (since\n",
      "each classification algorithm hides them in different places). To create\n",
      "this *analyzer* we can simply ask the classifier to do it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensana = fclf.get_sensitivity_analyzer()\n",
      "type(sensana)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "mvpa2.measures.base.MappedClassifierSensitivityAnalyzer"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "As you can see, this even works for our meta-classifier. And again this\n",
      "analyzer is a [processing object](http://pymvpa.org/glossary.html#term-processing-object) that returns the desired sensitivity\n",
      "when called with a dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sens = sensana(ds)\n",
      "type(sens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "mvpa2.datasets.base.Dataset"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sens.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(28, 39912)\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Why do we get 28 sensitivity maps from the classifier? The support vector\n",
      "machine constructs a model for binary classification problems. To be able to deal\n",
      "with this 8-category dataset, the data is internally split into all\n",
      "possible binary problems (there are exactly 28 of them). The sensitivities\n",
      "are extracted for all these partial problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "Figure out which sensitivity map belongs to which combination of\n",
      "categories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise\n",
      "print np.where([t == ('house', 'face') for t in sens.sa.targets])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(array([18]),)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "\n",
      "\n",
      "If you are not interested in this level of detail, we can combine the maps\n",
      "into one, as we have done with dataset samples before. A feasible\n",
      "algorithm might be to take the per feature maximum of absolute\n",
      "sensitivities in any of the maps. The resulting map will be an indication\n",
      "of the importance of feature for *some* partial classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sens_comb = sens.get_mapped(maxofabs_sample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "Project this sensitivity map back into the fMRI volume and compare it to\n",
      "the searchlight maps of different radii from the \n",
      "*previous tutorial*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise\n",
      "img = map2nifti(ds, data = sens_comb)\n",
      "img.to_filename('/tmp/sensitivity.nii.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "\n",
      "\n",
      "You might have noticed some imperfection in our recent approach to computing\n",
      "a full-brain sensitivity map. We derived it from the full dataset, and not\n",
      "from cross-validation splits of the data. Rectifying this is easy with a\n",
      "meta-measure. A meta-measure is analogous to a meta-classifier: a measure\n",
      "that takes a basic measure, adds a processing step to it and behaves like a\n",
      "measure itself. The meta-measure we want to use is\n",
      "[RepeatedMeasure](http://pymvpa.org/generated/mvpa2.measures.base.RepeatedMeasure.html#mvpa2-measures-base-repeatedmeasure)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensana = fclf.get_sensitivity_analyzer(postproc=maxofabs_sample())\n",
      "cv_sensana = RepeatedMeasure(sensana,\n",
      "                             ChainNode((NFoldPartitioner(),\n",
      "                                        Splitter('partitions',\n",
      "                                                 attr_values=(1,)))))\n",
      "sens = cv_sensana(ds)\n",
      "print sens.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(12, 39912)\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "We re-create our basic sensitivity analyzer, this time automatically applying\n",
      "the post-processing step that combines the sensitivity maps for all partial\n",
      "classifications. Finally, we plug it into the meta-measure that uses the\n",
      "partitions generated by [NFoldPartitioner](http://pymvpa.org/generated/mvpa2.datasets.splitters.NFoldPartitioner.html#mvpa2-datasets-splitters-nfoldpartitioner) to\n",
      "extract the training portions of the dataset for each fold. Afterwards, we can\n",
      "run the analyzer and we get another dataset, this time with a sensitivity map\n",
      "per each cross-validation split.\n",
      "\n",
      "We could combine these maps in a similar way as before, but let's look at\n",
      "the stability of the ANOVA feature selection instead."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ov = MapOverlap()\n",
      "overlap_fraction = ov(sens.samples > 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "With the [MapOverlap](http://pymvpa.org/generated/mvpa2.misc.support.MapOverlap.html#mvpa2-misc-support-mapoverlap) helper we can easily\n",
      "compute the fraction of features that have non-zero sensitivities in all\n",
      "dataset splits."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "Inspect the `ov` object. Access that statistics map with the fraction\n",
      "of per-feature selections across all splits and project them back into\n",
      "the fMRI volume to investigate them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise\n",
      "img = map2nifti(ds, data=ov.ovstats_map)\n",
      "img.to_filename('/tmp/fraction_splits.nii.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "\n",
      "\n",
      "This could be the end of the data processing. However, by using the meta\n",
      "measure to compute the sensitivity maps we have lost a convenient way to\n",
      "access the total performance of the underlying classifier. To again gain\n",
      "access to it, and get the sensitivities at the same time, we can twist the\n",
      "processing pipeline a bit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sclf = SplitClassifier(fclf, enable_ca=['stats'])\n",
      "cv_sensana = sclf.get_sensitivity_analyzer()\n",
      "sens = cv_sensana(ds)\n",
      "print sens.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(336, 39912)\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cv_sensana.clf.ca.stats.matrix  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 5  0  3  0  0  3  0  2]\n",
        " [ 0  9  0  0  0  0  0  0]\n",
        " [ 0  2  4  0  0  1  0  0]\n",
        " [ 2  1  0 12  0  0  0  0]\n",
        " [ 0  0  0  0 12  0  0  0]\n",
        " [ 3  0  4  0  0  6  2  1]\n",
        " [ 0  0  1  0  0  0 10  0]\n",
        " [ 2  0  0  0  0  2  0  9]]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "I guess that deserves some explanation. We wrap our\n",
      "[FeatureSelectionClassifier](http://pymvpa.org/generated/mvpa2.clfs.meta.FeatureSelectionClassifier.html#mvpa2-clfs-meta-featureselectionclassifier) with a new thing, a\n",
      "[SplitClassifier](http://pymvpa.org/generated/mvpa2.clfs.meta.SplitClassifier.html#mvpa2-clfs-meta-splitclassifier). This is another meta classifier\n",
      "that performs splitting of a dataset and runs training (and prediction) on\n",
      "each of the dataset splits separately. It can effectively perform a\n",
      "cross-validation analysis internally, and we ask it to compute a confusion\n",
      "matrix of it. The next step is to get a sensitivity analyzer for this meta\n",
      "meta classifier (this time no post-processing). Once we have got that, we\n",
      "can run the analysis and obtain sensitivity maps from all internally\n",
      "trained classifiers. Moreover, the meta sensitivity analyzer also allows\n",
      "access to its internal meta meta classifier that provides us with the\n",
      "confusion statistics. Yeah!\n",
      "\n",
      "While we are at it, it is worth mentioning that the scenario above can be\n",
      "further extended. We could add more selection or pre-processing steps\n",
      "into the classifier, like projecting the data onto PCA components and\n",
      "limit the classifier to the first 10 components -- for each split. PyMVPA\n",
      "offers even more complex meta classifiers (e.g.\n",
      "[TreeClassifier](http://pymvpa.org/generated/mvpa2.clfs.meta.TreeClassifier.html#mvpa2-clfs-meta-treeclassifier)) that might be very helpful in some\n",
      "analysis scenarios."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Closing Words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "We have seen that sensitivity analyses are a useful approach to localize\n",
      "information that is less constrained and less demanding than a searchlight\n",
      "analysis.  Specifically, we can use it to discover signals that are\n",
      "distributed throughout the whole set of features (e.g. the full brain),\n",
      "but we could also perform an ROI-based analysis with it. It is less\n",
      "computationally demanding as we only train the classifier on one set of\n",
      "features and not thousands, which results in a significant reduction of\n",
      "required CPU time.\n",
      "\n",
      "However, there are also caveats. While sensitivities are a much more\n",
      "direct measure of feature importance in the constructed model, being\n",
      "close to the bare metal of classifiers also has problems. Depending on the\n",
      "actual classification algorithm and data preprocessing sensitivities might mean something\n",
      "completely different when compared across classifiers. For example, the\n",
      "popular SVM algorithm solves the classification problem by identifying the\n",
      "data samples that are *most tricky* to model. The extracted sensitivities\n",
      "reflect this property. Other algorithms, such as \"Gaussian Naive Bayes\"\n",
      "([GNB](http://pymvpa.org/generated/mvpa2.clfs.gnb.GNB.html#mvpa2-clfs-gnb-gnb)) make assumptions about the distribution of\n",
      "the samples in each category. GNB sensitivities *might* look completely\n",
      "different, even if GNB and SVM classifiers both perform at comparable accuracy levels.\n",
      "Note, however, that these properties can also be used to address related\n",
      "research questions.\n",
      "\n",
      "It should also be noted that sensitivities can not be directly compared to\n",
      "each other, even if they stem from the same algorithm and are just\n",
      "computed on different dataset splits. In an analysis one would have to\n",
      "normalize them first. PyMVPA offers, for example,\n",
      "[l1_normed()](http://pymvpa.org/generated/mvpa2.misc.transformers.l1_normed.html#mvpa2-misc-transformers-l1-normed) and\n",
      "[l2_normed()](http://pymvpa.org/generated/mvpa2.misc.transformers.l2_normed.html#mvpa2-misc-transformers-l2-normed) that can be used in conjunction\n",
      "with [FxMapper](http://pymvpa.org/generated/mvpa2.mappers.fx.FxMapper.html#mvpa2-mappers-fx-fxmapper) to do that as a post-processing\n",
      "step.\n",
      "\n",
      "In this tutorial part we also touched the surface of another important\n",
      "topic: [feature selection](http://pymvpa.org/glossary.html#term-feature-selection). We performed an ANOVA-based feature\n",
      "selection prior to classification to help SVM achieve acceptable\n",
      "performance. One might wonder if that was a clever idea, since a\n",
      "*univariate* feature selection step prior to a *multivariate* analysis\n",
      "somewhat contradicts the goal to identify *multivariate* signals. Only\n",
      "features will be retained that show some signal on their own. If that\n",
      "turns out to be a problem for a particular analysis, PyMVPA offers a\n",
      "number of multivariate alternatives for features selection. There is an\n",
      "implementation of [recursive feature selection](http://pymvpa.org/glossary.html#term-recursive-feature-selection)\n",
      "([RFE](http://pymvpa.org/generated/mvpa2.featsel.rfe.RFE.html#mvpa2-featsel-rfe-rfe)), and also all classifier sensitivities\n",
      "can be used to select features. For classifiers where sensitivities cannot\n",
      "easily be extracted PyMVPA provides a noise perturbation measure\n",
      "([NoisePerturbationSensitivity](http://pymvpa.org/generated/mvpa2.measures.noiseperturbation.NoisePerturbationSensitivity.html#mvpa2-measures-noiseperturbation-noiseperturbationsensitivity);\n",
      "see *Hanson et al. (2004)* for an example application).\n",
      "\n",
      "With these building blocks it is possible to run fairly complex analyses.\n",
      "However, interpreting the results might not always be straight-forward. In\n",
      "the *next tutorial part* we will set out\n",
      "to take away another constraint of all our previously performed analyses. We\n",
      "are going to go beyond spatial analyses and explore the time dimension."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}